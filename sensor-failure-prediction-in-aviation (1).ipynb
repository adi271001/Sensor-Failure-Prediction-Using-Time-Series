{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-17T17:24:52.955271Z","iopub.execute_input":"2022-04-17T17:24:52.956053Z","iopub.status.idle":"2022-04-17T17:24:54.040578Z","shell.execute_reply.started":"2022-04-17T17:24:52.955935Z","shell.execute_reply":"2022-04-17T17:24:54.037743Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Loading Data","metadata":{}},{"cell_type":"code","source":"# define filepath to read data\ndir_path = '../input/nasa-cmaps/CMaps/'\n\n# define column names for easy indexing\nindex_names = ['unit_nr', 'time_cycles']\nsetting_names = ['setting_1', 'setting_2', 'setting_3']\nsensor_names = ['s_{}'.format(i) for i in range(1,22)] \ncol_names = index_names + setting_names + sensor_names\n\n# read data\ntrain = pd.read_csv((dir_path+'train_FD001.txt'), sep='\\s+', header=None, names=col_names)\ntest = pd.read_csv((dir_path+'test_FD001.txt'), sep='\\s+', header=None, names=col_names)\ny_test = pd.read_csv((dir_path+'RUL_FD001.txt'), sep='\\s+', header=None, names=['RUL'])\n\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.042381Z","iopub.execute_input":"2022-04-17T17:24:54.042728Z","iopub.status.idle":"2022-04-17T17:24:54.310386Z","shell.execute_reply.started":"2022-04-17T17:24:54.042687Z","shell.execute_reply":"2022-04-17T17:24:54.309713Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"# inspect unit_nr\ntrain[index_names].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.311914Z","iopub.execute_input":"2022-04-17T17:24:54.312370Z","iopub.status.idle":"2022-04-17T17:24:54.336360Z","shell.execute_reply.started":"2022-04-17T17:24:54.312332Z","shell.execute_reply":"2022-04-17T17:24:54.335616Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# inspect time cycles\ntrain[index_names].groupby('unit_nr').max().describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.339895Z","iopub.execute_input":"2022-04-17T17:24:54.340602Z","iopub.status.idle":"2022-04-17T17:24:54.367752Z","shell.execute_reply.started":"2022-04-17T17:24:54.340556Z","shell.execute_reply":"2022-04-17T17:24:54.367015Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# inspect settings\ntrain[setting_names].describe()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.368874Z","iopub.execute_input":"2022-04-17T17:24:54.369268Z","iopub.status.idle":"2022-04-17T17:24:54.407628Z","shell.execute_reply.started":"2022-04-17T17:24:54.369231Z","shell.execute_reply":"2022-04-17T17:24:54.407004Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# inspect sensor values\ntrain[sensor_names].describe().transpose()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.411401Z","iopub.execute_input":"2022-04-17T17:24:54.413366Z","iopub.status.idle":"2022-04-17T17:24:54.488915Z","shell.execute_reply.started":"2022-04-17T17:24:54.413328Z","shell.execute_reply":"2022-04-17T17:24:54.488278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def add_remaining_useful_life(df):\n    # Get the total number of cycles for each unit\n    grouped_by_unit = df.groupby(by=\"unit_nr\")\n    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n    \n    # Merge the max cycle back into the original frame\n    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n    \n    # Calculate remaining useful life for each row\n    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n    result_frame[\"RUL\"] = remaining_useful_life\n    \n    # drop max_cycle as it's no longer needed\n    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n    return result_frame\n\ntrain = add_remaining_useful_life(train)\ntrain[index_names+['RUL']].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.489985Z","iopub.execute_input":"2022-04-17T17:24:54.491089Z","iopub.status.idle":"2022-04-17T17:24:54.519276Z","shell.execute_reply.started":"2022-04-17T17:24:54.491049Z","shell.execute_reply":"2022-04-17T17:24:54.518613Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Plotting","metadata":{}},{"cell_type":"code","source":"# distribution of RUL, similar to the 'describe function' of time_cycles above, but visual\ndf_max_rul = train[['unit_nr', 'RUL']].groupby('unit_nr').max().reset_index()\ndf_max_rul['RUL'].hist(bins=15, figsize=(15,7))\nplt.xlabel('RUL')\nplt.ylabel('frequency')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.520330Z","iopub.execute_input":"2022-04-17T17:24:54.521052Z","iopub.status.idle":"2022-04-17T17:24:54.836891Z","shell.execute_reply.started":"2022-04-17T17:24:54.521008Z","shell.execute_reply":"2022-04-17T17:24:54.836215Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def plot_sensor(sensor_name):\n    plt.figure(figsize=(13,5))\n    for i in train['unit_nr'].unique():\n        if (i % 10 == 0):  # only plot every 10th unit_nr\n            plt.plot('RUL', sensor_name, \n                     data=train[train['unit_nr']==i])\n    plt.xlim(250, 0)  # reverse the x-axis so RUL counts down to zero\n    plt.xticks(np.arange(0, 275, 25))\n    plt.ylabel(sensor_name)\n    plt.xlabel('Remaining Use fulLife')\n    plt.show()\n\nfor sensor_name in sensor_names:\n    plot_sensor(sensor_name)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:24:54.838237Z","iopub.execute_input":"2022-04-17T17:24:54.838741Z","iopub.status.idle":"2022-04-17T17:25:00.516384Z","shell.execute_reply.started":"2022-04-17T17:24:54.838701Z","shell.execute_reply":"2022-04-17T17:25:00.515767Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Baseline Regression","metadata":{}},{"cell_type":"code","source":"# first create an evaluate function\ndef evaluate(y_true, y_hat, label='test'):\n    mse = mean_squared_error(y_true, y_hat)\n    rmse = np.sqrt(mse)\n    variance = r2_score(y_true, y_hat)\n    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:00.519507Z","iopub.execute_input":"2022-04-17T17:25:00.519995Z","iopub.status.idle":"2022-04-17T17:25:00.525021Z","shell.execute_reply.started":"2022-04-17T17:25:00.519957Z","shell.execute_reply":"2022-04-17T17:25:00.524201Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# drop unwanted columns and split target variable from training set\ndrop_sensors = ['s_1','s_5','s_6','s_10','s_16','s_18','s_19']\ndrop_labels = index_names+setting_names+drop_sensors\n\nX_train = train.drop(drop_labels, axis=1)\ny_train = X_train.pop('RUL')\n\n# Since the true RUL values for the test set are only provided for the last time cycle of each enginge, \n# the test set is subsetted to represent the same\nX_test = test.groupby('unit_nr').last().reset_index().drop(drop_labels, axis=1)\n\n\nprint(X_train.columns)  # check remaining columns","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:00.526449Z","iopub.execute_input":"2022-04-17T17:25:00.526993Z","iopub.status.idle":"2022-04-17T17:25:00.550340Z","shell.execute_reply.started":"2022-04-17T17:25:00.526955Z","shell.execute_reply":"2022-04-17T17:25:00.549483Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# create and fit model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# predict and evaluate\ny_hat_train = lm.predict(X_train)\nevaluate(y_train, y_hat_train, 'train')\n\ny_hat_test = lm.predict(X_test)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:00.552015Z","iopub.execute_input":"2022-04-17T17:25:00.552587Z","iopub.status.idle":"2022-04-17T17:25:00.590639Z","shell.execute_reply.started":"2022-04-17T17:25:00.552548Z","shell.execute_reply":"2022-04-17T17:25:00.589961Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1,1, figsize=(13,5))\n\nsignal = ax1.plot('RUL', 's_12', 'b',\n                 data=train.loc[train['unit_nr']==20])\nplt.xlim(250, 0)  # reverse the x-axis so RUL counts down to zero\nplt.xticks(np.arange(0, 275, 25))\nax1.set_ylabel('s_12', labelpad=20)\nax1.set_xlabel('RUL', labelpad=20)\n\nax2 = ax1.twinx()\nrul_line = ax2.plot('RUL', 'RUL', 'k', linewidth=4,\n                   data=train.loc[train['unit_nr']==20])\nax2.set_ylabel('RUL', labelpad=20)\n\n# code to have equal spacing of y ticks for both axes, so the gridlines allign\n# from https://stackoverflow.com/questions/20243683/matplotlib-align-twinx-tick-marks?rq=1\nax2.set_ylim(0, 250)  # set limits of axis you want to display neatly\nax2.set_yticks(\n    np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 6))  # choose integer to neatly divide your axis, in our case 6\nax1.set_yticks(\n    np.linspace(ax1.get_ybound()[0], ax1.get_ybound()[1], 6))  # apply same spacing to other axis\n\n# code to have a unified legend\n# from https://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend\nlines = signal+rul_line\nlabels = [line.get_label() for line in lines]\nax1.legend(lines, labels, loc=0)\n\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:00.592199Z","iopub.execute_input":"2022-04-17T17:25:00.592644Z","iopub.status.idle":"2022-04-17T17:25:01.010779Z","shell.execute_reply.started":"2022-04-17T17:25:00.592608Z","shell.execute_reply":"2022-04-17T17:25:01.010130Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Fine Tuning RUL","metadata":{}},{"cell_type":"code","source":"def add_remaining_useful_life(df):\n    # Get the total number of cycles for each unit\n    grouped_by_unit = df.groupby(by=\"unit_nr\")\n    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n    \n    # Merge the max cycle back into the original frame\n    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n    \n    # Calculate remaining useful life for each row\n    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n    result_frame[\"RUL\"] = remaining_useful_life\n    \n    # drop max_cycle as it's no longer needed\n    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n    return result_frame\n\ntrain = add_remaining_useful_life(train)\ntrain[index_names+['RUL']].head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.011847Z","iopub.execute_input":"2022-04-17T17:25:01.012217Z","iopub.status.idle":"2022-04-17T17:25:01.040631Z","shell.execute_reply.started":"2022-04-17T17:25:01.012182Z","shell.execute_reply":"2022-04-17T17:25:01.039989Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"clipped_rul = train.loc[train['unit_nr']==20].copy()\nclipped_rul['RUL'].clip(upper=125, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.041742Z","iopub.execute_input":"2022-04-17T17:25:01.042428Z","iopub.status.idle":"2022-04-17T17:25:01.049003Z","shell.execute_reply.started":"2022-04-17T17:25:01.042391Z","shell.execute_reply":"2022-04-17T17:25:01.048256Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"fig, ax1 = plt.subplots(1,1, figsize=(13,5))\n\nsignal = ax1.plot('RUL', 's_12', 'b',\n                 data=train.loc[train['unit_nr']==20])\nplt.xlim(250, 0)  # reverse the x-axis so RUL counts down to zero\nplt.xticks(np.arange(0, 275, 25))\nax1.set_ylabel('s_12', labelpad=20)\nax1.set_xlabel('RUL', labelpad=20)\n\nax2 = ax1.twinx()\nrul_line = ax2.plot('RUL', 'RUL', 'k', linewidth=4,\n                   data=train.loc[train['unit_nr']==20])\nrul = train.loc[train['unit_nr']==20, 'RUL']\nrul_line2 = ax2.plot(rul, rul.where(rul <= 125, 125), '--g', linewidth=4, label='clipped_rul')  # SET LABEL MANUALLY?\nax2.set_ylabel('RUL', labelpad=20)\n\n# code to have equal spacing of y ticks for both axes, so the gridlines allign\n# from https://stackoverflow.com/questions/20243683/matplotlib-align-twinx-tick-marks?rq=1\nax2.set_ylim(0, 250)  # set limits of axis you want to display neatly\nax2.set_yticks(\n    np.linspace(ax2.get_ybound()[0], ax2.get_ybound()[1], 6))  # choose integer to neatly divide your axis, in our case 6\nax1.set_yticks(\n    np.linspace(ax1.get_ybound()[0], ax1.get_ybound()[1], 6))  # apply same spacing to other axis\n\n\n# code to have a unified legend\n# from https://stackoverflow.com/questions/5484922/secondary-axis-with-twinx-how-to-add-to-legend\nlines = signal+rul_line+rul_line2\nlabels = [line.get_label() for line in lines]\nax1.legend(lines, labels, loc=0)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.050683Z","iopub.execute_input":"2022-04-17T17:25:01.050966Z","iopub.status.idle":"2022-04-17T17:25:01.525879Z","shell.execute_reply.started":"2022-04-17T17:25:01.050915Z","shell.execute_reply":"2022-04-17T17:25:01.525225Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# prep data\n# drop unwanted columns and split target variable from training set\ndrop_sensors = ['s_1','s_5','s_6','s_10','s_16','s_18','s_19']\ndrop_labels = index_names+setting_names+drop_sensors\n\nX_train = train.drop(drop_labels, axis=1)\ny_train = X_train.pop('RUL')\n\n# Since the true RUL values for the test set are only provided for the last time cycle of each enginge, \n# the test set is subsetted to represent the same\nX_test = test.groupby('unit_nr').last().reset_index().drop(drop_labels, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.526954Z","iopub.execute_input":"2022-04-17T17:25:01.527317Z","iopub.status.idle":"2022-04-17T17:25:01.540253Z","shell.execute_reply.started":"2022-04-17T17:25:01.527282Z","shell.execute_reply":"2022-04-17T17:25:01.539509Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_hat, label='test'):\n    mse = mean_squared_error(y_true, y_hat)\n    rmse = np.sqrt(mse)\n    variance = r2_score(y_true, y_hat)\n    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.541375Z","iopub.execute_input":"2022-04-17T17:25:01.541681Z","iopub.status.idle":"2022-04-17T17:25:01.549105Z","shell.execute_reply.started":"2022-04-17T17:25:01.541644Z","shell.execute_reply":"2022-04-17T17:25:01.548371Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"y_train_clipped = y_train.clip(upper=125)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.550136Z","iopub.execute_input":"2022-04-17T17:25:01.550328Z","iopub.status.idle":"2022-04-17T17:25:01.559387Z","shell.execute_reply.started":"2022-04-17T17:25:01.550299Z","shell.execute_reply":"2022-04-17T17:25:01.558727Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# I previously used the where method, but .clip seems more intuitive\nall(y_train.where(y_train <= 125, 125) == y_train_clipped)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.560884Z","iopub.execute_input":"2022-04-17T17:25:01.561184Z","iopub.status.idle":"2022-04-17T17:25:01.572385Z","shell.execute_reply.started":"2022-04-17T17:25:01.561145Z","shell.execute_reply":"2022-04-17T17:25:01.571620Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"## Clipped RUL\n# create and fit model\nlm = LinearRegression()\nlm.fit(X_train, y_train_clipped)  \n\n# predict and evaluate\ny_hat_train = lm.predict(X_train)\nevaluate(y_train_clipped, y_hat_train, 'train')\n\ny_hat_test = lm.predict(X_test)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.573692Z","iopub.execute_input":"2022-04-17T17:25:01.573935Z","iopub.status.idle":"2022-04-17T17:25:01.613474Z","shell.execute_reply.started":"2022-04-17T17:25:01.573902Z","shell.execute_reply":"2022-04-17T17:25:01.612787Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# SVR","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, r2_score","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.617461Z","iopub.execute_input":"2022-04-17T17:25:01.620223Z","iopub.status.idle":"2022-04-17T17:25:01.638667Z","shell.execute_reply.started":"2022-04-17T17:25:01.620180Z","shell.execute_reply":"2022-04-17T17:25:01.637738Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# SVM initial regression + clipped RUL\nsvr = SVR(kernel='linear')\nsvr.fit(X_train, y_train_clipped)  \n\n# predict and evaluate\ny_hat_train = svr.predict(X_train)\nevaluate(y_train_clipped, y_hat_train, 'train')\n\ny_hat_test = svr.predict(X_test)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:01.642894Z","iopub.execute_input":"2022-04-17T17:25:01.645430Z","iopub.status.idle":"2022-04-17T17:25:39.990792Z","shell.execute_reply.started":"2022-04-17T17:25:01.645387Z","shell.execute_reply":"2022-04-17T17:25:39.990010Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Scaling","metadata":{}},{"cell_type":"code","source":"# Scaling\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nscaler = MinMaxScaler()\n# scaler = StandardScaler()  # for this specific dataset, the type of scaler does not have any effect\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:39.992291Z","iopub.execute_input":"2022-04-17T17:25:39.992765Z","iopub.status.idle":"2022-04-17T17:25:40.008325Z","shell.execute_reply.started":"2022-04-17T17:25:39.992727Z","shell.execute_reply":"2022-04-17T17:25:40.007630Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# SVM regression + clipped RUL + scaled\nsvr = SVR(kernel='linear')\nsvr.fit(X_train_scaled, y_train_clipped)  \n# predict and evaluate\ny_hat_train = svr.predict(X_train_scaled)\nevaluate(y_train_clipped, y_hat_train, 'train')\ny_hat_test = svr.predict(X_test_scaled)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:25:40.009682Z","iopub.execute_input":"2022-04-17T17:25:40.010180Z","iopub.status.idle":"2022-04-17T17:26:07.780438Z","shell.execute_reply.started":"2022-04-17T17:25:40.010142Z","shell.execute_reply":"2022-04-17T17:26:07.779716Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Feature engineering\nfrom sklearn.preprocessing import PolynomialFeatures\n# 2nd degree polynomialFeatures of [a, b] becomes [1, a, b, a^2, ab, b^2]\npoly = PolynomialFeatures(2)\nX_train_transformed = poly.fit_transform(X_train_scaled)\nX_test_transformed = poly.fit_transform(X_test_scaled)\n\nprint(X_train_scaled.shape)\nprint(X_train_transformed.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:26:07.781584Z","iopub.execute_input":"2022-04-17T17:26:07.783096Z","iopub.status.idle":"2022-04-17T17:26:07.806980Z","shell.execute_reply.started":"2022-04-17T17:26:07.783054Z","shell.execute_reply":"2022-04-17T17:26:07.806271Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# SVM regression + clipped RUL + engineered features\nsvr_f = SVR(kernel='linear')\nsvr_f.fit(X_train_transformed, y_train_clipped)  \n\n# predict and evaluate\ny_hat_train = svr_f.predict(X_train_transformed)\nevaluate(y_train_clipped, y_hat_train, 'train')\n\ny_hat_test = svr_f.predict(X_test_transformed)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:26:07.808305Z","iopub.execute_input":"2022-04-17T17:26:07.808772Z","iopub.status.idle":"2022-04-17T17:26:58.577793Z","shell.execute_reply.started":"2022-04-17T17:26:07.808734Z","shell.execute_reply":"2022-04-17T17:26:58.577082Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Feature Selection","metadata":{}},{"cell_type":"code","source":"# Feature engineering + selection\nfrom sklearn.feature_selection import SelectFromModel\nselect_features = SelectFromModel(svr_f, threshold='mean', prefit=True)\nselect_features.get_support()\nfeature_names = poly.get_feature_names()\n\nprint('Original features:\\n', X_train.columns)\nprint('Best features:\\n', np.array(feature_names)[select_features.get_support()])\nnp.array(feature_names)[select_features.get_support()].shape","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:26:58.583283Z","iopub.execute_input":"2022-04-17T17:26:58.583491Z","iopub.status.idle":"2022-04-17T17:26:58.666206Z","shell.execute_reply.started":"2022-04-17T17:26:58.583465Z","shell.execute_reply":"2022-04-17T17:26:58.662845Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# SVM regression + clipped RUL + engineered features + selection\nsvr = SVR(kernel='linear')\nsvr.fit(X_train_transformed[:, select_features.get_support()], y_train_clipped) \n\n# predict and evaluate\ny_hat_train = svr.predict(X_train_transformed[:, select_features.get_support()])\nevaluate(y_train_clipped, y_hat_train, 'train')\n\ny_hat_test = svr.predict(X_test_transformed[:, select_features.get_support()])\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:26:58.667312Z","iopub.execute_input":"2022-04-17T17:26:58.667896Z","iopub.status.idle":"2022-04-17T17:27:28.380012Z","shell.execute_reply.started":"2022-04-17T17:26:58.667859Z","shell.execute_reply":"2022-04-17T17:27:28.379207Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# Simple Hyper parameter Tuning","metadata":{}},{"cell_type":"code","source":"epsilon = [0.4, 0.3, 0.2, 0.1, 0.05]\n\nfor e in epsilon:\n    svr = SVR(kernel='linear', epsilon=e)\n    svr.fit(X_train_transformed[:, select_features.get_support()], y_train_clipped)\n\n    # predict and evaluate\n    y_hat = svr.predict(X_train_transformed[:, select_features.get_support()])\n    mse = mean_squared_error(y_train_clipped, y_hat)\n    rmse = np.sqrt(mse)\n    variance = r2_score(y_train_clipped, y_hat)\n    print(\"epsilon:\", e, \"RMSE:\", rmse, \"R2:\", variance)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:27:28.381144Z","iopub.execute_input":"2022-04-17T17:27:28.381791Z","iopub.status.idle":"2022-04-17T17:30:01.869473Z","shell.execute_reply.started":"2022-04-17T17:27:28.381752Z","shell.execute_reply":"2022-04-17T17:30:01.868422Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"epsilon value 0.2 is giving the best RMSE","metadata":{}},{"cell_type":"markdown","source":"# Final Model","metadata":{}},{"cell_type":"code","source":"svr = SVR(kernel='linear', epsilon=0.2)\nsvr.fit(X_train_transformed[:, select_features.get_support()], y_train_clipped)\n\n# predict and evaluate\ny_hat_train = svr.predict(X_train_transformed[:, select_features.get_support()])\nevaluate(y_train_clipped, y_hat_train, 'train')\n\ny_hat_test = svr.predict(X_test_transformed[:, select_features.get_support()])\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:01.871238Z","iopub.execute_input":"2022-04-17T17:30:01.871546Z","iopub.status.idle":"2022-04-17T17:30:32.813817Z","shell.execute_reply.started":"2022-04-17T17:30:01.871503Z","shell.execute_reply":"2022-04-17T17:30:32.812728Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"The final SVR has a test RMSE of 20.54. The combination of updating our assumption of RUL and fitting an SVR with tuned boundaries, feature scaling and polynomial features provides a 35.7 % improvement over our baseline model (RMSE = 31.95).","metadata":{}},{"cell_type":"markdown","source":"# Time Series","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.815345Z","iopub.execute_input":"2022-04-17T17:30:32.815885Z","iopub.status.idle":"2022-04-17T17:30:32.824691Z","shell.execute_reply.started":"2022-04-17T17:30:32.815845Z","shell.execute_reply":"2022-04-17T17:30:32.823900Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_hat, label='test'):\n    mse = mean_squared_error(y_true, y_hat)\n    rmse = np.sqrt(mse)\n    variance = r2_score(y_true, y_hat)\n    print('{} set RMSE:{}, Variance:{}'.format(label, rmse, variance))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.825874Z","iopub.execute_input":"2022-04-17T17:30:32.826581Z","iopub.status.idle":"2022-04-17T17:30:32.833766Z","shell.execute_reply.started":"2022-04-17T17:30:32.826537Z","shell.execute_reply":"2022-04-17T17:30:32.833012Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# drop unwanted columns based on Exploratory Data Analysis conducted earlier\ndrop_sensors = ['s_1','s_5','s_6','s_10','s_16','s_18','s_19']\ndrop_labels = setting_names+drop_sensors\n\nX_train = train.drop(drop_labels, axis=1)\n# y_train = X_train.pop('RUL')  # pop RUL after dropping NaNs introduced by creating lagged variables\n\nX_test_interim = test.drop(drop_labels, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.834981Z","iopub.execute_input":"2022-04-17T17:30:32.835856Z","iopub.status.idle":"2022-04-17T17:30:32.845196Z","shell.execute_reply.started":"2022-04-17T17:30:32.835818Z","shell.execute_reply":"2022-04-17T17:30:32.844467Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"# Lagged variable","metadata":{}},{"cell_type":"code","source":"# create lagged variables\nremaining_sensors = X_train.columns.difference(index_names+['RUL'])\nlag1 = [col + '_lag_1' for col in remaining_sensors]\n\nX_train[lag1] = X_train.groupby('unit_nr')[remaining_sensors].shift(1)\nX_train.dropna(inplace=True)\n\nX_test_interim[lag1] = X_test_interim.groupby('unit_nr')[remaining_sensors].shift(1)\nX_test_interim.dropna(inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.846368Z","iopub.execute_input":"2022-04-17T17:30:32.847278Z","iopub.status.idle":"2022-04-17T17:30:32.894458Z","shell.execute_reply.started":"2022-04-17T17:30:32.847243Z","shell.execute_reply":"2022-04-17T17:30:32.893793Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"X_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.895910Z","iopub.execute_input":"2022-04-17T17:30:32.896177Z","iopub.status.idle":"2022-04-17T17:30:32.925694Z","shell.execute_reply.started":"2022-04-17T17:30:32.896143Z","shell.execute_reply":"2022-04-17T17:30:32.924838Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# target variable\ny_train = X_train.pop('RUL')\n\n# prep test data, since the true RUL values for the test set are only provided for the last time cycle\n# of each enginge, the test set is subsetted to represent the same\nX_test = X_test_interim.groupby('unit_nr').last().reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.927129Z","iopub.execute_input":"2022-04-17T17:30:32.927503Z","iopub.status.idle":"2022-04-17T17:30:32.938008Z","shell.execute_reply.started":"2022-04-17T17:30:32.927468Z","shell.execute_reply":"2022-04-17T17:30:32.937079Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# create and fit model\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# predict and evaluate\ny_hat_train = lm.predict(X_train)\nevaluate(y_train, y_hat_train, 'train')\n\ny_hat_test = lm.predict(X_test)\nevaluate(y_test, y_hat_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.939406Z","iopub.execute_input":"2022-04-17T17:30:32.939662Z","iopub.status.idle":"2022-04-17T17:30:32.991600Z","shell.execute_reply.started":"2022-04-17T17:30:32.939629Z","shell.execute_reply":"2022-04-17T17:30:32.990839Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# Adding Multiple Lags","metadata":{}},{"cell_type":"code","source":"def add_lagged_variables(df_input, nr_of_lags, columns):\n    df = df_input.copy()\n    for i in range(nr_of_lags):\n        lagged_columns = [col + '_lag_{}'.format(i+1) for col in columns]\n        df[lagged_columns] = df.groupby('unit_nr')[columns].shift(i+1)\n    df.dropna(inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:32.992743Z","iopub.execute_input":"2022-04-17T17:30:32.993871Z","iopub.status.idle":"2022-04-17T17:30:33.009652Z","shell.execute_reply.started":"2022-04-17T17:30:32.993835Z","shell.execute_reply":"2022-04-17T17:30:33.008967Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def add_specific_lags(df_input, list_of_lags, columns):\n    df = df_input.copy()\n    for i in list_of_lags:\n        lagged_columns = [col + '_lag_{}'.format(i) for col in columns]\n        df[lagged_columns] = df.groupby('unit_nr')[columns].shift(i)\n    df.dropna(inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:33.014229Z","iopub.execute_input":"2022-04-17T17:30:33.015833Z","iopub.status.idle":"2022-04-17T17:30:33.029063Z","shell.execute_reply.started":"2022-04-17T17:30:33.015793Z","shell.execute_reply":"2022-04-17T17:30:33.028085Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# Stationarity","metadata":{}},{"cell_type":"code","source":"# stationarity graphs\nt = np.arange(0,150)\nfactor = 0.2\nstationary = np.sin(factor*t)\nincreasing_mean = np.sin(factor*t) + t/100\nincreasing_variance = np.sin(factor*t) * (1 + t/100)\ninconsistent_covariance = np.sin((factor + t/500) * t)\n\nplotlist = [stationary, increasing_mean, increasing_variance, inconsistent_covariance]\nplotnames = ['stationary', 'increasing_mean', 'increasing_variance', 'inconsistent_covariance']  # not very elegant but gets the job done\n\nplt.subplots(2, 2, figsize=(12,7))  # initiate subplot figure\nylim = 2.5\n\nfor i in range(len(plotlist)):\n\n    plt.subplot(2, 2, i+1)  # define which subplot to fill, range starts at 0 so increment with 1\n    plt.plot(t, plotlist[i], linewidth=3)\n    plt.ylim(-ylim, ylim)\n    plt.tick_params(which='both', bottom=False, labelbottom=False, left=False, labelleft=False)  # remove axis ticks and labels\n    plt.title(str(plotnames[i]))\n\nplt.tight_layout(pad=3)  # specify layout and whitespace padding between graphs\nplt.show()\n# plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:33.030800Z","iopub.execute_input":"2022-04-17T17:30:33.031721Z","iopub.status.idle":"2022-04-17T17:30:33.498441Z","shell.execute_reply.started":"2022-04-17T17:30:33.031647Z","shell.execute_reply":"2022-04-17T17:30:33.497771Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# test stationarity using adfuller test\nfrom statsmodels.tsa.stattools import adfuller\n\nadf, pvalue, usedlag, n_obs, critical_values, icbest = adfuller(train[ 's_7'], maxlag=1)\nprint('all units, single column test results')\nprint('adf: {} \\npvalue: {}'.format(adf, pvalue))\nprint('Significant: {}'.format(pvalue < 0.05))\nprint('NOTE: Testing one column has values of 100 engines, all engines together are stationary, but single engines are not!\\n')\n\ntest_series = train.loc[train['unit_nr']==1, 's_7']\nadf, pvalue, usedlag, n_obs, critical_values, icbest = adfuller(test_series, maxlag=1)\nprint('single unit, single column test results')\nprint('adf: {} \\npvalue: {}'.format(adf, pvalue))\nprint('Significant: {}\\n'.format(pvalue < 0.05))\n\ntest_series = test_series.diff(1).dropna()\nadf, pvalue, usedlag, n_obs, critical_values, icbest = adfuller(test_series, maxlag=1)\nprint('single unit, single column test results after differencing')\nprint('adf: {} \\npvalue: {}'.format(adf, pvalue))\nprint('Significant: {}\\n'.format(pvalue < 0.05))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:33.499749Z","iopub.execute_input":"2022-04-17T17:30:33.500013Z","iopub.status.idle":"2022-04-17T17:30:33.678732Z","shell.execute_reply.started":"2022-04-17T17:30:33.499978Z","shell.execute_reply":"2022-04-17T17:30:33.675070Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# full column vs single unit\nplt.subplots(3,1, figsize=(15,8))\n\nplt.subplot(3,1,1)\nplt.plot(train[ 's_7'])\nplt.title('full column, appears stationary \\n\\\nadf=-32.41, pvalue=0.0')\n\n\nplt.subplot(3,1,2)\nplt.plot(train.loc[train['unit_nr']==1, 's_7'])\nplt.title('individual engines are not stationary (engine unit_nr 1) \\n\\\nadf=-2.28, pvalue=0.18')\n\n\nplt.subplot(3,1,3)\nplt.plot(train.loc[train['unit_nr']==1, 's_7'].diff(1).dropna())\nplt.title('differenced individual engines can be stationary (engine unit_nr 1) \\n\\\nadf=-14.82, pvalue<0.01')\n\n\nplt.tight_layout()\nplt.show()\n# plt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:33.680087Z","iopub.execute_input":"2022-04-17T17:30:33.680397Z","iopub.status.idle":"2022-04-17T17:30:34.374356Z","shell.execute_reply.started":"2022-04-17T17:30:33.680362Z","shell.execute_reply":"2022-04-17T17:30:34.372795Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# test code to find the maximum times a time series needs to be differenced\nunit = train.loc[train['unit_nr']==1].drop(drop_labels + ['unit_nr', 'time_cycles', 'RUL'], axis=1) # subset to unit nr 1 and sensors of interest.\nfor col in unit.columns:\n    maxdiff = 0\n    do = True\n    adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(unit[col], maxlag=1)\n    if pvalue < 0.05:\n        do = False\n    \n    while do:\n        maxdiff += 1\n        adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(unit[col].diff(maxdiff).dropna(), maxlag=1)\n        if pvalue < 0.05:  # if significant, stop differencing and testing for stationarity\n            do = False\n \n    print(\"{}: pvalue = {}, maxdiff = {}\".format(col, pvalue, maxdiff))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:34.375865Z","iopub.execute_input":"2022-04-17T17:30:34.376395Z","iopub.status.idle":"2022-04-17T17:30:34.438291Z","shell.execute_reply.started":"2022-04-17T17:30:34.376356Z","shell.execute_reply":"2022-04-17T17:30:34.437612Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# wrap the code to find the max difference in a function and create a function to make the time-series stationary\ndef find_max_diff(series):\n    maxdiff = 0\n    do = True\n    adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(series, maxlag=1)\n    if pvalue < 0.05:\n        do = False\n    \n    while do:\n        maxdiff += 1\n        adf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(series.diff(maxdiff).dropna(), maxlag=1)\n        if pvalue < 0.05:  # if significant, stop differencing and testing for stationarity\n            do = False\n    return maxdiff\n\n\ndef make_stationary(df_input, columns):\n    df = df_input.copy()\n    for unit_nr in range(1, df['unit_nr'].max()+1):\n        for col in columns:\n            maxdiff = find_max_diff(df.loc[df['unit_nr']==unit_nr, col])\n            if maxdiff > 0:\n                df.loc[df['unit_nr']==unit_nr, col] = df.loc[df['unit_nr']==unit_nr, col].diff(maxdiff)\n    df.dropna(inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:34.439782Z","iopub.execute_input":"2022-04-17T17:30:34.440265Z","iopub.status.idle":"2022-04-17T17:30:34.450384Z","shell.execute_reply.started":"2022-04-17T17:30:34.440227Z","shell.execute_reply":"2022-04-17T17:30:34.449605Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# make all remaining sensors stationary per unit_nr\nintermediate_df = train.drop(drop_labels, axis=1)\nintermediate_df = make_stationary(intermediate_df, remaining_sensors)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:34.451875Z","iopub.execute_input":"2022-04-17T17:30:34.452160Z","iopub.status.idle":"2022-04-17T17:30:38.733341Z","shell.execute_reply.started":"2022-04-17T17:30:34.452101Z","shell.execute_reply":"2022-04-17T17:30:38.732612Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"intermediate_df.head()  # stationary data!","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:38.734764Z","iopub.execute_input":"2022-04-17T17:30:38.735019Z","iopub.status.idle":"2022-04-17T17:30:38.758475Z","shell.execute_reply.started":"2022-04-17T17:30:38.734984Z","shell.execute_reply":"2022-04-17T17:30:38.757591Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# create and fit model\nlm = LinearRegression()\nlm.fit(intermediate_df[remaining_sensors], intermediate_df['RUL'])\n\n# predict and evaluate\ny_hat_train = lm.predict(intermediate_df[remaining_sensors])\nevaluate(intermediate_df['RUL'], y_hat_train, 'train')","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:38.760021Z","iopub.execute_input":"2022-04-17T17:30:38.760329Z","iopub.status.idle":"2022-04-17T17:30:38.789639Z","shell.execute_reply.started":"2022-04-17T17:30:38.760293Z","shell.execute_reply":"2022-04-17T17:30:38.788979Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Training RMSE has become quite a bit worse as a result of making the data stationary. This can be explained by the data no longer having a trend, making it difficult to fit a regression line. Scores will improve again when adding lagged variables, as this will inform the model of the change of the sensor values over time","metadata":{}},{"cell_type":"markdown","source":"# Searching the best number of lags required","metadata":{}},{"cell_type":"code","source":"# add lags and evaluate models to find optimal lag length\nimport statsmodels.api as sm\n\nmetrics = pd.DataFrame(columns=['rmse', 'AIC', 'BIC'])\nnr_of_lags = 30\nfor i in range(0, nr_of_lags+1):\n    X_train = add_lagged_variables(intermediate_df, i, remaining_sensors)\n    X_train = X_train.drop(index_names, axis=1)\n    y_train = X_train.pop('RUL')\n    \n    model = sm.OLS(y_train, sm.add_constant(X_train.values))\n    result = model.fit()\n\n    metrics = metrics.append(pd.DataFrame(data=[[np.sqrt(result.mse_resid), round(result.aic,2), round(result.bic,2)]],\n                               columns=['rmse', 'AIC', 'BIC']),\n                               ignore_index = True)\n\ndisplay(metrics)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:30:38.791060Z","iopub.execute_input":"2022-04-17T17:30:38.792116Z","iopub.status.idle":"2022-04-17T17:31:15.653412Z","shell.execute_reply.started":"2022-04-17T17:30:38.792081Z","shell.execute_reply":"2022-04-17T17:31:15.652730Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(metrics['AIC'].diff(), marker='.')  # plot the difference to see where it flattens out\nplt.plot(14, metrics['AIC'].diff()[14], '.r')\nplt.xlabel(\"Nr of lags\")\nplt.ylabel(\"AIC rate of change\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:31:15.654660Z","iopub.execute_input":"2022-04-17T17:31:15.655044Z","iopub.status.idle":"2022-04-17T17:31:15.887708Z","shell.execute_reply.started":"2022-04-17T17:31:15.655000Z","shell.execute_reply":"2022-04-17T17:31:15.887007Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"result.summary()  # Check warnings and condition number at the bottom of output","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:31:15.888916Z","iopub.execute_input":"2022-04-17T17:31:15.889298Z","iopub.status.idle":"2022-04-17T17:31:16.225476Z","shell.execute_reply.started":"2022-04-17T17:31:15.889262Z","shell.execute_reply":"2022-04-17T17:31:16.224818Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Adding multiple lags to same variable leads to multi collinearity","metadata":{}},{"cell_type":"markdown","source":"# Multi Collinearity","metadata":{}},{"cell_type":"code","source":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nX_train = add_lagged_variables(intermediate_df, 14, remaining_sensors)\nX_train = X_train.drop(index_names, axis=1)\n\n# note, this takes a few minutes to calculate\nvifs = {X_train.columns[i]:round(vif(X_train.values, i), 2) for i in range(len(X_train.columns))}\ndisplay(vifs)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:31:16.226866Z","iopub.execute_input":"2022-04-17T17:31:16.227390Z","iopub.status.idle":"2022-04-17T17:35:08.296004Z","shell.execute_reply.started":"2022-04-17T17:31:16.227351Z","shell.execute_reply":"2022-04-17T17:35:08.295037Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# add scaling to hopefully prevent high multicolinearity\nfrom sklearn.preprocessing import StandardScaler\n\nintermediate_df = train.drop(drop_labels, axis=1)\nscaler = StandardScaler()\nscaler.fit(intermediate_df[remaining_sensors])\nintermediate_df[remaining_sensors] = scaler.transform(intermediate_df[remaining_sensors])\n\nintermediate_df = make_stationary(intermediate_df, remaining_sensors)\n\nX_train = add_lagged_variables(intermediate_df, 14, remaining_sensors)\nX_train = X_train.drop(index_names, axis=1)\n\nvifs = {X_train.columns[i]:round(vif(X_train.values, i), 2) for i in range(len(X_train.columns))}\ndisplay(vifs)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:35:08.297320Z","iopub.execute_input":"2022-04-17T17:35:08.298038Z","iopub.status.idle":"2022-04-17T17:39:05.577466Z","shell.execute_reply.started":"2022-04-17T17:35:08.297980Z","shell.execute_reply":"2022-04-17T17:39:05.576662Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate And Predict","metadata":{}},{"cell_type":"code","source":"# first let's re-examine the optimal amount of lags as we did before\n# execute data processing steps to make sure we're working with the correctly processed data\nintermediate_df = train.drop(drop_labels, axis=1)\nscaler = StandardScaler()\nscaler.fit(intermediate_df[remaining_sensors])\nintermediate_df[remaining_sensors] = scaler.transform(intermediate_df[remaining_sensors])\n\nintermediate_df = make_stationary(intermediate_df, remaining_sensors)\n\n# calculate metrics\nmetrics = pd.DataFrame(columns=['rmse', 'AIC', 'BIC'])\nnr_of_lags = 30\nfor i in range(0, nr_of_lags+1):\n    X_train = add_lagged_variables(intermediate_df, i, remaining_sensors)\n    X_train = X_train.drop(index_names, axis=1)\n    y_train = X_train.pop('RUL')\n    \n    model = sm.OLS(y_train, sm.add_constant(X_train.values))\n    result = model.fit()\n\n    metrics = metrics.append(pd.DataFrame(data=[[np.sqrt(result.mse_resid), round(result.aic,2), round(result.bic,2)]],\n                               columns=['rmse', 'AIC', 'BIC']),\n                               ignore_index = True)\n\ndisplay(metrics)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:39:05.578626Z","iopub.execute_input":"2022-04-17T17:39:05.579047Z","iopub.status.idle":"2022-04-17T17:39:45.510882Z","shell.execute_reply.started":"2022-04-17T17:39:05.579009Z","shell.execute_reply":"2022-04-17T17:39:45.510161Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(metrics['AIC'].diff(), marker='.')  # plot the difference to see where it flattens out\nplt.plot(9, metrics['AIC'].diff()[9], '.r')\nplt.xlabel(\"Nr of lags\")\nplt.ylabel(\"AIC rate of change\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:39:45.512200Z","iopub.execute_input":"2022-04-17T17:39:45.512630Z","iopub.status.idle":"2022-04-17T17:39:45.812646Z","shell.execute_reply.started":"2022-04-17T17:39:45.512593Z","shell.execute_reply":"2022-04-17T17:39:45.811778Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# train and evaluate model with 0 to n lags\nlags = 9\n\n# prep data\nX_train_interim = train.drop(drop_labels, axis=1)\nX_train_interim[remaining_sensors] = scaler.transform(X_train_interim[remaining_sensors])\nX_train_interim = make_stationary(X_train_interim, remaining_sensors)\nX_train_interim = add_lagged_variables(X_train_interim, lags, remaining_sensors)\nX_train_interim = sm.add_constant(X_train_interim)\nX_train = X_train_interim.drop(index_names, axis=1)\ny_train = X_train.pop(\"RUL\")\n\nX_test_interim = test.drop(drop_labels, axis=1)\nX_test_interim[remaining_sensors] = scaler.transform(X_test_interim[remaining_sensors])\nX_test_interim = make_stationary(X_test_interim, remaining_sensors)\nX_test_interim = add_lagged_variables(X_test_interim, lags, remaining_sensors)\nX_test_interim = X_test_interim.groupby('unit_nr').last().reset_index()\nX_test_interim = sm.add_constant(X_test_interim)\nX_test = X_test_interim.drop(index_names, axis=1)\n\n# fit model\nmodel = sm.OLS(y_train.clip(upper=125), X_train)  # apply clipped RUL from last post\nmodel_fitted = model.fit()\n\n# predict\ny_hat_train = model_fitted.predict(X_train)\ny_hat = model_fitted.predict(X_test)\n\n# evaluate\nevaluate(y_train.clip(upper=125), y_hat_train, 'train')\nevaluate(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:39:45.814024Z","iopub.execute_input":"2022-04-17T17:39:45.814440Z","iopub.status.idle":"2022-04-17T17:39:53.526351Z","shell.execute_reply.started":"2022-04-17T17:39:45.814405Z","shell.execute_reply":"2022-04-17T17:39:53.525638Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# train and evaluate model with specific lags\nspecific_lags = [1,2,3,4,5,10,20]\n\n# prep data\nX_train_interim = train.drop(drop_labels, axis=1)\nX_train_interim[remaining_sensors] = scaler.transform(X_train_interim[remaining_sensors])\nX_train_interim = make_stationary(X_train_interim, remaining_sensors)\nX_train_interim = add_specific_lags(X_train_interim, specific_lags, remaining_sensors)\nX_train_interim = sm.add_constant(X_train_interim)\nX_train = X_train_interim.drop(index_names, axis=1)\ny_train = X_train.pop(\"RUL\")\n\nX_test_interim = test.drop(drop_labels, axis=1)\nX_test_interim[remaining_sensors] = scaler.transform(X_test_interim[remaining_sensors])\nX_test_interim = make_stationary(X_test_interim, remaining_sensors)\nX_test_interim = add_specific_lags(X_test_interim, specific_lags, remaining_sensors)\nX_test_interim = X_test_interim.groupby('unit_nr').last().reset_index()\nX_test_interim = sm.add_constant(X_test_interim)\nX_test = X_test_interim.drop(index_names, axis=1)\n\n# fit model\nmodel = sm.OLS(y_train.clip(upper=125), X_train)\nmodel_fitted = model.fit()\n\n# predict\ny_hat_train = model_fitted.predict(X_train)\ny_hat = model_fitted.predict(X_test)\n\n# evaluate\nevaluate(y_train.clip(upper=125), y_hat_train, 'train')\nevaluate(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:39:53.527489Z","iopub.execute_input":"2022-04-17T17:39:53.527956Z","iopub.status.idle":"2022-04-17T17:40:01.458517Z","shell.execute_reply.started":"2022-04-17T17:39:53.527903Z","shell.execute_reply":"2022-04-17T17:40:01.449791Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"# Results\n* specific lags and test rmse\n* rmse = 21.7642, lags = [1,2,3,4,5]\n* rmse = 21.1489, lags = [1,2,3,4,5,6,7,8,9]\n* rmse = 21.0690, lags = [1,2,3,4,5,10,20,30]\n* rmse = 20.8522, lags = [1,2,3,4,5,10,20]","metadata":{}},{"cell_type":"markdown","source":"# Survival Model","metadata":{}},{"cell_type":"code","source":"pip install lifelines\n","metadata":{"execution":{"iopub.status.busy":"2022-04-17T17:40:01.459697Z","iopub.execute_input":"2022-04-17T17:40:01.460016Z","iopub.status.idle":"2022-04-17T17:40:14.480631Z","shell.execute_reply.started":"2022-04-17T17:40:01.459977Z","shell.execute_reply":"2022-04-17T17:40:14.479707Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom lifelines import KaplanMeierFitter, CoxTimeVaryingFitter","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:03:03.690926Z","iopub.execute_input":"2022-04-17T18:03:03.691649Z","iopub.status.idle":"2022-04-17T18:03:03.699514Z","shell.execute_reply.started":"2022-04-17T18:03:03.691616Z","shell.execute_reply":"2022-04-17T18:03:03.698840Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# define filepaths to read data\ndir_path = '../input/nasa-cmaps/CMaps/'\ntrain_file = 'train_FD001.txt'\ntest_file = 'test_FD001.txt'\n\n# define column names for easy indexing\nindex_names = ['unit_nr', 'time_cycles']\nsetting_names = ['setting_1', 'setting_2', 'setting_3']\nsensor_names = ['s_{}'.format(i) for i in range(1,22)] \ncol_names = index_names + setting_names + sensor_names\n\n# read data\ntrain = pd.read_csv((dir_path+train_file), sep='\\s+', header=None, \n                 names=col_names)\nX_test = pd.read_csv((dir_path+test_file), sep='\\s+', header=None, \n                 names=col_names)\ny_test = pd.read_csv((dir_path+'RUL_FD001.txt'), sep='\\s+', header=None, \n                 names=['RemainingUsefulLife'])","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:05:59.249530Z","iopub.execute_input":"2022-04-17T18:05:59.249781Z","iopub.status.idle":"2022-04-17T18:05:59.396683Z","shell.execute_reply.started":"2022-04-17T18:05:59.249751Z","shell.execute_reply":"2022-04-17T18:05:59.395994Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:00.863875Z","iopub.execute_input":"2022-04-17T18:06:00.864643Z","iopub.status.idle":"2022-04-17T18:06:00.898901Z","shell.execute_reply.started":"2022-04-17T18:06:00.864602Z","shell.execute_reply":"2022-04-17T18:06:00.898245Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"def add_remaining_useful_life(df):\n    # Get the total number of cycles for each unit\n    grouped_by_unit = df.groupby(by=\"unit_nr\")\n    max_cycle = grouped_by_unit[\"time_cycles\"].max()\n    \n    # Merge the max cycle back into the original frame\n    result_frame = df.merge(max_cycle.to_frame(name='max_cycle'), left_on='unit_nr', right_index=True)\n    \n    # Calculate remaining useful life for each row\n    remaining_useful_life = result_frame[\"max_cycle\"] - result_frame[\"time_cycles\"]\n    result_frame[\"RUL\"] = remaining_useful_life\n    \n    # drop max_cycle as it's no longer needed\n    result_frame = result_frame.drop(\"max_cycle\", axis=1)\n    return result_frame\n\ntrain = add_remaining_useful_life(train)  # add computed RUL for later use\ndisplay(train[index_names+['RUL']].head())","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:32.244027Z","iopub.execute_input":"2022-04-17T18:06:32.244527Z","iopub.status.idle":"2022-04-17T18:06:32.275520Z","shell.execute_reply.started":"2022-04-17T18:06:32.244488Z","shell.execute_reply":"2022-04-17T18:06:32.274861Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"train['RUL'].clip(upper=125, inplace=True)  # clip RUL as discussed in SVR and problem framing analysis\n\ndrop_sensors = ['s_1','s_5','s_6','s_10','s_16','s_18','s_19']  # non-informative features, derived from EDA\ndrop_labels = setting_names + drop_sensors\ntrain.drop(labels=drop_labels, axis=1, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:34.302562Z","iopub.execute_input":"2022-04-17T18:06:34.302811Z","iopub.status.idle":"2022-04-17T18:06:34.313366Z","shell.execute_reply.started":"2022-04-17T18:06:34.302782Z","shell.execute_reply":"2022-04-17T18:06:34.312444Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"print(train.columns)\nremaining_sensors = ['s_2', 's_3', 's_4', 's_7', 's_8', 's_9',\n       's_11', 's_12', 's_13', 's_14', 's_15', 's_17', 's_20', 's_21']","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:36.148452Z","iopub.execute_input":"2022-04-17T18:06:36.149241Z","iopub.status.idle":"2022-04-17T18:06:36.156361Z","shell.execute_reply.started":"2022-04-17T18:06:36.149201Z","shell.execute_reply":"2022-04-17T18:06:36.155600Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"# Preparing The Data","metadata":{}},{"cell_type":"code","source":"train['breakdown'] = 0\nidx_last_record = train.reset_index().groupby(by='unit_nr')['index'].last()  # engines breakdown at the last cycle\ntrain.at[idx_last_record, 'breakdown'] = 1","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:40.151014Z","iopub.execute_input":"2022-04-17T18:06:40.151462Z","iopub.status.idle":"2022-04-17T18:06:40.165491Z","shell.execute_reply.started":"2022-04-17T18:06:40.151426Z","shell.execute_reply":"2022-04-17T18:06:40.164617Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"train['start'] = train['time_cycles'] - 1 \ntrain.tail() # check results","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:41.699738Z","iopub.execute_input":"2022-04-17T18:06:41.700391Z","iopub.status.idle":"2022-04-17T18:06:41.732610Z","shell.execute_reply.started":"2022-04-17T18:06:41.700354Z","shell.execute_reply":"2022-04-17T18:06:41.731936Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"# Right Sensoring","metadata":{}},{"cell_type":"code","source":"cut_off = 200\ntrain_censored = train[train['time_cycles'] <= cut_off].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:45.274669Z","iopub.execute_input":"2022-04-17T18:06:45.274914Z","iopub.status.idle":"2022-04-17T18:06:45.286789Z","shell.execute_reply.started":"2022-04-17T18:06:45.274887Z","shell.execute_reply":"2022-04-17T18:06:45.285719Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"# Kaplan Meier Curve","metadata":{}},{"cell_type":"code","source":"# create kaplan meier curve\ndata = train_censored[index_names+['breakdown']].groupby('unit_nr').last()\n\nplt.figure(figsize=(15,7))\nsurvival = KaplanMeierFitter()\nsurvival.fit(data['time_cycles'], data['breakdown'])\nsurvival.plot()\nplt.ylabel(\"Probability of survival\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:48.127287Z","iopub.execute_input":"2022-04-17T18:06:48.127547Z","iopub.status.idle":"2022-04-17T18:06:48.432372Z","shell.execute_reply.started":"2022-04-17T18:06:48.127518Z","shell.execute_reply":"2022-04-17T18:06:48.430027Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"markdown","source":"The KaplanMeier curve provides us with some initial information of survival probabilities. For example, engines have a 100% probability of surviving the first 128 time_cycles. After that point the first engines start to break down, but there is still a 46% probability of the engine surviving past 200 time_cycles.","metadata":{}},{"cell_type":"markdown","source":"# CoxTimeVaryingFitter","metadata":{}},{"cell_type":"markdown","source":"The CoxTimeVaryingFitter Which can leverage the timeseries nature of the data, as it is able to take multiple observations for each engine into account. The downside to this model is, its results are less intuitive to interpret. In general, higher partial hazards indicate a higher risk of failure, but this is no direct indication of time till event.","metadata":{}},{"cell_type":"code","source":"train_cols = index_names + remaining_sensors + ['start', 'breakdown']\npredict_cols = ['time_cycles'] + remaining_sensors + ['start', 'breakdown']  # breakdown value will be 0","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:52.610316Z","iopub.execute_input":"2022-04-17T18:06:52.610995Z","iopub.status.idle":"2022-04-17T18:06:52.616739Z","shell.execute_reply.started":"2022-04-17T18:06:52.610950Z","shell.execute_reply":"2022-04-17T18:06:52.615965Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"ctv = CoxTimeVaryingFitter()\nctv.fit(train_censored[train_cols], id_col=\"unit_nr\", event_col='breakdown', \n        start_col='start', stop_col='time_cycles', show_progress=True, step_size=1)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:53.382671Z","iopub.execute_input":"2022-04-17T18:06:53.383351Z","iopub.status.idle":"2022-04-17T18:06:53.577241Z","shell.execute_reply.started":"2022-04-17T18:06:53.383309Z","shell.execute_reply":"2022-04-17T18:06:53.576392Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"ctv.print_summary()\nplt.figure(figsize=(10,5))\nctv.plot()\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:06:57.734533Z","iopub.execute_input":"2022-04-17T18:06:57.734800Z","iopub.status.idle":"2022-04-17T18:06:58.088379Z","shell.execute_reply.started":"2022-04-17T18:06:57.734768Z","shell.execute_reply":"2022-04-17T18:06:58.084288Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"display(ctv.summary)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:07:02.608034Z","iopub.execute_input":"2022-04-17T18:07:02.608562Z","iopub.status.idle":"2022-04-17T18:07:02.647989Z","shell.execute_reply.started":"2022-04-17T18:07:02.608522Z","shell.execute_reply":"2022-04-17T18:07:02.647018Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"# Predict And Evaluate","metadata":{}},{"cell_type":"code","source":"df = train_censored.groupby(\"unit_nr\").last()\ndf = df[df['breakdown'] == 0]  # get engines from dataset which are still functioning so we can predict their RUL\ndf_to_predict = df[df['breakdown'] == 0].copy()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:07:06.131309Z","iopub.execute_input":"2022-04-17T18:07:06.132081Z","iopub.status.idle":"2022-04-17T18:07:06.147342Z","shell.execute_reply.started":"2022-04-17T18:07:06.132042Z","shell.execute_reply":"2022-04-17T18:07:06.146393Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"predictions['predictions'] = ctv.predict_log_partial_hazard(df_to_predict[predict_cols])\ndf_last = train.groupby('unit_nr').last()\npredictions['RUL'] = df_to_predict['RUL']\npredictions.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:07:13.741208Z","iopub.execute_input":"2022-04-17T18:07:13.741767Z","iopub.status.idle":"2022-04-17T18:07:13.772306Z","shell.execute_reply.started":"2022-04-17T18:07:13.741727Z","shell.execute_reply":"2022-04-17T18:07:13.771497Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,5))\nplt.plot(predictions['RUL'], predictions['predictions'], '.b')\nxlim = plt.gca().get_xlim()\nplt.xlim(xlim[1], xlim[0])\nplt.xlabel('RUL')\nplt.ylabel('log_partial_hazard')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:07:15.852340Z","iopub.execute_input":"2022-04-17T18:07:15.852595Z","iopub.status.idle":"2022-04-17T18:07:16.083845Z","shell.execute_reply.started":"2022-04-17T18:07:15.852567Z","shell.execute_reply":"2022-04-17T18:07:16.083141Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"# Predicitng Hazards","metadata":{}},{"cell_type":"code","source":"df_hazard = train_censored.copy()\ndf_hazard['hazard'] = ctv.predict_log_partial_hazard(df_hazard)\ndf_hazard.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:08:20.140022Z","iopub.execute_input":"2022-04-17T18:08:20.140536Z","iopub.status.idle":"2022-04-17T18:08:20.198182Z","shell.execute_reply.started":"2022-04-17T18:08:20.140494Z","shell.execute_reply":"2022-04-17T18:08:20.197407Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"df_hazard.plot('hazard', 'RUL', 'scatter', figsize=(15,5))\nplt.xlabel('hazard')\nplt.ylabel('RUL')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:08:22.002480Z","iopub.execute_input":"2022-04-17T18:08:22.003145Z","iopub.status.idle":"2022-04-17T18:08:22.307008Z","shell.execute_reply.started":"2022-04-17T18:08:22.003107Z","shell.execute_reply":"2022-04-17T18:08:22.306029Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_hat, label='test'):\n    mse = mean_squared_error(y_true, y_hat)\n    rmse = np.sqrt(mse)\n    variance = r2_score(y_true, y_hat)\n    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:08:25.330272Z","iopub.execute_input":"2022-04-17T18:08:25.330974Z","iopub.status.idle":"2022-04-17T18:08:25.336663Z","shell.execute_reply.started":"2022-04-17T18:08:25.330915Z","shell.execute_reply":"2022-04-17T18:08:25.335838Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"from scipy.optimize import curve_fit\ndef exponential_model(z, a, b):\n    return a * np.exp(-b * z)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:39.888974Z","iopub.execute_input":"2022-04-17T18:20:39.889377Z","iopub.status.idle":"2022-04-17T18:20:39.894861Z","shell.execute_reply.started":"2022-04-17T18:20:39.889334Z","shell.execute_reply":"2022-04-17T18:20:39.893972Z"},"trusted":true},"execution_count":167,"outputs":[]},{"cell_type":"code","source":"y_hat = exponential_model(df_hazard.loc[df_hazard['unit_nr']==1, 'hazard'], 70, 0.1)\nplt.plot(df_hazard.loc[df_hazard['unit_nr']==1, 'hazard'], df_hazard.loc[df_hazard['unit_nr']==1, 'RUL'], 'o', df_hazard.loc[df_hazard['unit_nr']==1, 'hazard'], y_hat)\nplt.xlabel(\"log_partial_hazard\")\nplt.ylabel(\"RUL\")\nplt.show()\nplt.close()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:41.342868Z","iopub.execute_input":"2022-04-17T18:20:41.343462Z","iopub.status.idle":"2022-04-17T18:20:41.600525Z","shell.execute_reply.started":"2022-04-17T18:20:41.343422Z","shell.execute_reply":"2022-04-17T18:20:41.599817Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"popt, pcov = curve_fit(exponential_model, df_hazard['hazard'], df_hazard['RUL'])\npopt # the coefficients","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:43.626646Z","iopub.execute_input":"2022-04-17T18:20:43.626895Z","iopub.status.idle":"2022-04-17T18:20:43.646321Z","shell.execute_reply.started":"2022-04-17T18:20:43.626867Z","shell.execute_reply":"2022-04-17T18:20:43.645598Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"# prep test set\ntest = X_test.drop(labels=drop_labels, axis=1)\ntest['breakdown'] = 0\ntest['start'] = test['time_cycles'] - 1","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:54.910376Z","iopub.execute_input":"2022-04-17T18:20:54.910636Z","iopub.status.idle":"2022-04-17T18:20:54.919605Z","shell.execute_reply.started":"2022-04-17T18:20:54.910607Z","shell.execute_reply":"2022-04-17T18:20:54.918535Z"},"trusted":true},"execution_count":170,"outputs":[]},{"cell_type":"code","source":"y_hat.isna()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:56.433265Z","iopub.execute_input":"2022-04-17T18:20:56.433516Z","iopub.status.idle":"2022-04-17T18:20:56.443419Z","shell.execute_reply.started":"2022-04-17T18:20:56.433486Z","shell.execute_reply":"2022-04-17T18:20:56.442616Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"train.isna()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:57.722127Z","iopub.execute_input":"2022-04-17T18:20:57.722382Z","iopub.status.idle":"2022-04-17T18:20:57.762081Z","shell.execute_reply.started":"2022-04-17T18:20:57.722354Z","shell.execute_reply":"2022-04-17T18:20:57.761341Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"df_hazard['RUL'].isna()","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:58.310859Z","iopub.execute_input":"2022-04-17T18:20:58.311437Z","iopub.status.idle":"2022-04-17T18:20:58.321839Z","shell.execute_reply.started":"2022-04-17T18:20:58.311397Z","shell.execute_reply":"2022-04-17T18:20:58.320984Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"# predict and evaluate\ny_hat = exponential_model(df_hazard['hazard'], *popt)\nevaluate(df_hazard['RUL'], y_hat.fillna(0), 'train')\ny_pred = ctv.predict_log_partial_hazard(test.groupby('unit_nr').last())\ny_hat = exponential_model(y_pred, *popt)\nevaluate(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:20:58.990516Z","iopub.execute_input":"2022-04-17T18:20:58.990898Z","iopub.status.idle":"2022-04-17T18:20:59.018864Z","shell.execute_reply.started":"2022-04-17T18:20:58.990859Z","shell.execute_reply":"2022-04-17T18:20:59.018159Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"ctv2 = CoxTimeVaryingFitter()\nctv2.fit(train[train_cols], id_col=\"unit_nr\", event_col='breakdown', \n        start_col='start', stop_col='time_cycles', show_progress=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:21:19.805343Z","iopub.execute_input":"2022-04-17T18:21:19.805608Z","iopub.status.idle":"2022-04-17T18:21:20.046114Z","shell.execute_reply.started":"2022-04-17T18:21:19.805577Z","shell.execute_reply":"2022-04-17T18:21:20.045293Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"train['hazard'] = ctv2.predict_log_partial_hazard(train)\npopt2, pcov2 = curve_fit(exponential_model, train['hazard'], train['RUL'])\n\ny_hat = exponential_model(train['hazard'], *popt2)\nevaluate(train['RUL'], y_hat, 'train')\n\ny_pred = ctv2.predict_log_partial_hazard(test.groupby('unit_nr').last())\ny_hat = exponential_model(y_pred, *popt2)\nevaluate(y_test, y_hat)","metadata":{"execution":{"iopub.status.busy":"2022-04-17T18:21:34.467297Z","iopub.execute_input":"2022-04-17T18:21:34.467549Z","iopub.status.idle":"2022-04-17T18:21:34.719913Z","shell.execute_reply.started":"2022-04-17T18:21:34.467521Z","shell.execute_reply":"2022-04-17T18:21:34.719188Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"markdown","source":"Although it doesn't come as close to the SVR or time-series analysis solutions, part of the innaccuracy can be explained by fitting another model ontop of the predicted log_partial_hazard, which results in errors ontop of errors (as no model is perfect). I believe when you step away from the RUL paradigm we've been using and set a threshold for the log_partial_hazard this method could be very appropriate to define when maintenance is required.","metadata":{}}]}